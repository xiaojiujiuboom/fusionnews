import os
import requests
import feedparser
import datetime
import time
import random
import hashlib
import re
import google.generativeai as genai
from bs4 import BeautifulSoup
from time import mktime
from urllib.parse import urljoin

# --- é…ç½®éƒ¨åˆ† ---
SERVERCHAN_SENDKEY = os.environ.get("SERVERCHAN_SENDKEY")
GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY") 

# åˆå§‹åŒ– Gemini
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        # ä½¿ç”¨ 1.5-flash ä¿è¯é€Ÿåº¦å’Œç¨³å®šæ€§ (å¤„ç†é•¿æ–‡æœ¬èƒ½åŠ›å¼º)
        model = genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        print(f"Gemini é…ç½®å‡ºé”™: {e}")
else:
    print("è­¦å‘Š: æœªé…ç½® GOOGLE_API_KEY")

# --- è¾…åŠ©å‡½æ•°ï¼šé€šç”¨ç½‘é¡µæŠ“å– ---
def fetch_url(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp.text, resp.url
    except Exception:
        pass
    return None, None

# --- 1. è·å–æ–°é—» (ä¿æŒä¸å˜) ---
def get_fusion_news():
    print("æ­£åœ¨æŠ“å–æ–°é—»...")
    rss_url = "https://news.google.com/rss/search?q=Nuclear+Fusion+when:48h&hl=en-US&gl=US&ceid=US:en"
    try:
        feed = feedparser.parse(rss_url)
        news_items = []
        for entry in feed.entries[:6]: 
            published_time_str = "æœªçŸ¥æ—¶é—´"
            if hasattr(entry, 'published_parsed'):
                dt = datetime.datetime.fromtimestamp(mktime(entry.published_parsed))
                published_time_str = dt.strftime('%Y-%m-%d %H:%M')
            news_items.append(f"- {entry.title} (Time: {published_time_str}) [Link: {entry.link}]")
        return "\n".join(news_items) if news_items else "è¿‡å»48å°æ—¶æ— é‡å¤§æ–°é—»ã€‚"
    except Exception as e:
        return f"æ–°é—»æŠ“å–å¤±è´¥: {e}"

# --- 2. æ·±åº¦éå†èŒä½ (Deep Crawler) ---
def search_jobs_deep_dive():
    print("ğŸš€ å¯åŠ¨äºŒçº§æ·±åº¦çˆ¬è™« (Deep Traversal Mode)...")
    
    # ç›®æ ‡æºï¼šåŒ…å«èšåˆé¡µå’Œå…·ä½“çš„å®éªŒå®¤é¡µé¢
    # ç­–ç•¥ï¼šä¸ä»…çœ‹è¿™ä¸ªé¡µé¢ï¼Œè¿˜è¦å°è¯•ç‚¹å¼€é‡Œé¢çš„ç†å·¥ç§‘å²—ä½
    targets = [
        {"name": "ITER Jobs", "url": "https://www.iter.org/jobs", "base": "https://www.iter.org"},
        {"name": "UKAEA (è‹±å›½åŸå­èƒ½å±€)", "url": "https://careers.ukaea.uk/vacancies/", "base": "https://careers.ukaea.uk"},
        {"name": "Princeton Plasma Physics Lab", "url": "https://www.pppl.gov/careers", "base": "https://www.pppl.gov"},
        {"name": "General Fusion", "url": "https://generalfusion.com/careers/", "base": "https://generalfusion.com"},
        {"name": "Tokamak Energy", "url": "https://tokamakenergy.co.uk/careers", "base": "https://tokamakenergy.co.uk"},
        {"name": "Commonwealth Fusion Systems", "url": "https://cfs.energy/careers", "base": "https://cfs.energy"},
        {"name": "US Fusion Energy", "url": "https://usfusionenergy.org/opportunities", "base": "https://usfusionenergy.org"}
    ]

    # å…³é”®è¯è¿‡æ»¤ï¼šåªå¯¹åŒ…å«è¿™äº›è¯çš„é“¾æ¥æ„Ÿå…´è¶£
    stem_keywords = ["physicist", "engineer", "scientist", "research", "plasma", "postdoc", "fellow", "technical", "diagnostics", "magnet", "cryogenic"]
    
    final_report_data = ""

    # ä¸ºäº†é˜²æ­¢è¶…æ—¶ï¼Œæˆ‘ä»¬éšæœºé€‰ 3 ä¸ªæºè¿›è¡Œæ·±åº¦æ‰«æï¼Œè€Œä¸æ˜¯å…¨éƒ¨
    selected_targets = random.sample(targets, 3)

    for target in selected_targets:
        print(f"æ­£åœ¨æ‰«æ: {target['name']} ...")
        html, final_url = fetch_url(target['url'])
        
        if not html:
            continue
            
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. æå–é¡µé¢ä¸Šæ‰€æœ‰çš„é“¾æ¥
        links = soup.find_all('a', href=True)
        potential_jobs = []

        # 2. ç­›é€‰å‡ºå¯èƒ½æ˜¯â€œå…·ä½“å²—ä½â€çš„é“¾æ¥
        for link in links:
            text = link.get_text().lower()
            href = link['href']
            # å¦‚æœé“¾æ¥æ–‡æœ¬åŒ…å«ç†å·¥ç§‘å…³é”®è¯ï¼Œä¸”ä¸æ˜¯ä»…ä»…è·³è½¬å›ä¸»é¡µ
            if any(k in text for k in stem_keywords) and len(text) > 10:
                full_link = urljoin(target.get('base', ''), href)
                potential_jobs.append({"title": link.get_text().strip(), "url": full_link})

        # å»é‡
        potential_jobs = [dict(t) for t in {tuple(d.items()) for d in potential_jobs}]

        # 3. å¦‚æœæ‰¾åˆ°äº†å…·ä½“å²—ä½ï¼Œéšæœºé€‰ 1-2 ä¸ªç‚¹è¿›å»çœ‹çœ‹ (äºŒçº§éå†)
        if potential_jobs:
            print(f"  -> å‘ç° {len(potential_jobs)} ä¸ªæ½œåœ¨ç†å·¥ç§‘å²—ä½ï¼Œæ­£åœ¨æ·±å…¥åˆ†æå…¶ä¸­ 1-2 ä¸ª...")
            jobs_to_visit = random.sample(potential_jobs, min(2, len(potential_jobs)))
            
            for job in jobs_to_visit:
                print(f"    -> æ­£åœ¨æŠ“å–è¯¦æƒ…: {job['title'][:30]}...")
                job_html, _ = fetch_url(job['url'])
                if job_html:
                    job_soup = BeautifulSoup(job_html, 'html.parser')
                    # ç§»é™¤è„šæœ¬ï¼Œåªç•™æ­£æ–‡
                    for s in job_soup(["script", "style", "nav", "footer"]):
                        s.extract()
                    # æå–æ­£æ–‡ (é™åˆ¶é•¿åº¦)
                    job_text = job_soup.get_text(separator='\n', strip=True)[:3500]
                    
                    final_report_data += f"\n=== æ·±åº¦æŠ“å–: {target['name']} ===\n"
                    final_report_data += f"å²—ä½åç§°: {job['title']}\n"
                    final_report_data += f"é“¾æ¥: {job['url']}\n"
                    final_report_data += f"è¯¦æƒ…é¡µæ­£æ–‡:\n{job_text}\n"
                    final_report_data += "--------------------------------\n"
                    time.sleep(1) # ç¤¼è²Œå»¶æ—¶
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°å…·ä½“é“¾æ¥ï¼Œå°±æŠ“å½“å‰é¡µé¢çš„å¤§æ¦‚
            print("  -> æœªå‘ç°å…·ä½“å²—ä½é“¾æ¥ï¼Œä»…æŠ“å–æ¦‚è§ˆã€‚")
            text = soup.get_text(separator='\n', strip=True)[:2000]
            final_report_data += f"\n=== æ¦‚è§ˆ: {target['name']} ===\nå†…å®¹: {text}\né“¾æ¥: {target['url']}\n----------------\n"

    if not final_report_data:
        return "æœ¬æ¬¡æ·±åº¦æ‰«ææœªè·å–æœ‰æ•ˆæ•°æ®ï¼Œå»ºè®®ç›´æ¥è®¿é—® ITER æˆ– FIA å®˜ç½‘ã€‚"
        
    return final_report_data

# --- 3. ç”Ÿæˆæ—¥æŠ¥ (Prompt é’ˆå¯¹ JD/Quals ä¼˜åŒ–) ---
def generate_daily_report(news_text, job_data):
    print("æ­£åœ¨ç”Ÿæˆ AI æ—¥æŠ¥...")
    today_str = datetime.date.today().strftime('%Y-%m-%d')

    # ã€è¶…çº§ç¡¬æ ¸çŸ¥è¯†åº“ã€‘æ‹’ç»å®½æ³›ï¼Œç›´å‡»ç»†èŠ‚
    hardcore_topics = [
        "æ–°ç»å…¸æ’•è£‚æ¨¡ (Neoclassical Tearing Modes, NTM)", "ç”µé˜»å£æ¨¡ (Resistive Wall Modes, RWM)",
        "åˆ®å‰Šå±‚ (SOL) å®½åº¦ä¸çƒ­æµå¯†åº¦", "ELM ç¼“è§£çº¿åœˆ (RMP Coils)",
        "é’¨é“œåæ»¤å™¨å•ä½“è®¾è®¡ (W/Cu Monoblock)", "æ°šå¢æ®–åŒ…å±‚ (Tritium Breeding Blanket) çš„çƒ­å·¥æ°´åŠ›",
        "é«˜æ¸©è¶…å¯¼ç£ä½“å¤±è¶…ä¿æŠ¤ (Quench Protection)", "REBCO èƒ¶å¸¦çš„æœºæ¢°å‰¥ç¦»å¼ºåº¦",
        "ä¸­æ€§æŸæ³¨å…¥ (NBI) çš„æ°”ä½“ä¸­å’Œæ•ˆç‡", "ç”µå­å›æ—‹å…±æŒ¯åŠ çƒ­ (ECRH) çš„æˆªæ­¢å¯†åº¦",
        "æ¿€å…‰èšå˜ä¸­çš„ç‘åˆ©-æ³°å‹’ä¸ç¨³å®šæ€§ (Rayleigh-Taylor Instability)", "ç›´æ¥é©±åŠ¨ vs é—´æ¥é©±åŠ¨ (Direct vs Indirect Drive)",
        "æ¿€æ³¢ç‚¹ç« (Shock Ignition)", "ç£åŒ–çº¿æ€§æƒ¯æ€§èšå˜ (MagLIF)",
        "ä»¿æ˜Ÿå™¨çš„å‡†ç­‰è§’å¯¹ç§°æ€§ (Quasi-isodynamic symmetry)", "çƒå½¢æ‰˜å¡é©¬å…‹çš„ä½ç¯å¾„æ¯”ä¼˜åŠ¿",
        "æ¶²æ€é”‚ç¬¬ä¸€å£ (Liquid Lithium First Wall)", "èšå˜å †é¥æ“ä½œ (Remote Handling) çš„æŠ—è¾ç…§ç”µå­å­¦",
        "DEMO ååº”å †çš„åœå †å‰‚é‡ç‡", "èšå˜-è£‚å˜æ··åˆå †çš„æ¬¡ä¸´ç•Œå®‰å…¨æ€§"
    ]
    
    # åŸºäºæ—¥æœŸçš„å“ˆå¸Œé€‰æ‹©
    date_hash = int(hashlib.sha256(today_str.encode('utf-8')).hexdigest(), 16)
    today_topic = hardcore_topics[date_hash % len(hardcore_topics)]

    prompt = f"""
    ä½ æ˜¯ä¸€ä½**æ ¸èšå˜é¢†åŸŸçš„èµ„æ·±çŒå¤´å’ŒæŠ€æœ¯ä¸“å®¶**ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®ç”Ÿæˆ {today_str} çš„æƒ…æŠ¥æ—¥æŠ¥ã€‚
    
    ---
    ### 1. æ–°é—» (News)
    {news_text}
    
    ### 2. æ·±åº¦èŒä½æƒ…æŠ¥ (Deep Crawled Jobs)
    *(è¿™æ˜¯çˆ¬è™«è¿›å…¥å…·ä½“å²—ä½è¯¦æƒ…é¡µæŠ“å–çš„æ­£æ–‡ï¼Œå¯èƒ½åŒ…å«æ‚ä¹±ä¿¡æ¯ã€‚)*
    {job_data}
    
    ### 3. ä»Šæ—¥ç¡¬æ ¸è¯¾é¢˜: {today_topic}
    
    ---
    ### è¾“å‡ºæ ¼å¼è¦æ±‚ (Markdown)
    
    # âš›ï¸ èšå˜æƒ…æŠ¥å±€ | {today_str}
    
    ## ğŸ“° 1. Fusion Frontiers
    *(ç²¾é€‰ 4-5 æ¡æ–°é—»)*
    * **[ä¸­æ–‡æ ‡é¢˜]**
        * ğŸ•’ **Time**: [æ—¶é—´]
        * ğŸš€ **Significance**: [ç‚¹è¯„]
        * ğŸ”— [åŸæ–‡]({'{link}'})
    
    ## ğŸ¯ 2. Career Radar (ç†å·¥ç§‘æ·±åº¦åˆ†æ)
    *(æŒ‡ä»¤ï¼šè¯·åŸºäºã€æ·±åº¦æŠ“å–ã€‘çš„å†…å®¹ï¼Œæå–å‡ºå…·ä½“çš„å²—ä½ç¡¬æ ¸ä¿¡æ¯ã€‚)*
    *(æ ¼å¼è¦æ±‚ï¼šå¿…é¡»æå– Job Description å’Œ Qualificationsã€‚å¦‚æœæ­£æ–‡é‡Œæ²¡æœ‰ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡åˆç†æ¨æ–­ã€‚)*
    
    *(é’ˆå¯¹æ¯ä¸€ä¸ªæŠ“å–åˆ°çš„å…·ä½“å²—ä½)*
    * ğŸ¢ **[æœºæ„å]** â€”â€” **[å²—ä½åç§°]**
        * ğŸ“„ **Job Description (è¦åšä»€ä¹ˆ)**: 
            [è¯¦ç»†æ¦‚æ‹¬ï¼šæ¯”å¦‚è´Ÿè´£COMSOLä»¿çœŸã€çœŸç©ºå®¤è®¾è®¡ã€ç­‰ç¦»å­ä½“è¯Šæ–­ä»£ç ç¼–å†™ç­‰]
        * ğŸ“ **Qualifications (è¦æ±‚ä»€ä¹ˆ)**: 
            [ç¡¬æ€§æŒ‡æ ‡ï¼šæ¯”å¦‚ PhD in Physics, Python/C++, ç†Ÿæ‚‰ Ansys, 3å¹´çœŸç©ºç»éªŒç­‰]
        * ğŸ”— [ç‚¹å‡»ç›´è¾¾å²—ä½]({'{link}'})

    *(å¦‚æœæŠ“å–çš„æ˜¯æ¦‚è§ˆè€Œéå…·ä½“å²—ä½ï¼Œè¯·ç®€è¦æ€»ç»“æœºæ„æ‹›è˜åŠ¨å‘)*
    
    ## ğŸ§  3. Deep Dive: {today_topic}
    *(ä»Šå¤©å¿…é¡»è®²è¿™ä¸ªå…·ä½“çš„ç»†åˆ†é¢†åŸŸï¼Œä¸è¦è®²å®½æ³›çš„æ¦‚å¿µï¼)*
    * **ä»Šæ—¥è¯æ¡ï¼š{today_topic}**
    * **ğŸ§ ç¡¬æ ¸ç‰©ç†/å·¥ç¨‹è§£æ**ï¼š
        [250å­—ã€‚è¯·ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä¾‹å¦‚ç£é¢ã€é€šé‡ã€æ²‰ç§¯ã€æˆªé¢ç­‰ï¼Œå±•ç¤ºæ·±åº¦ã€‚]
    * **ğŸ ä¹Ÿå°±æ˜¯äººè¯ç‰ˆ**ï¼š
        [150å­—ã€‚ç”¨æå…¶ç²¾å¦™çš„ç±»æ¯”ï¼ˆå¦‚æ°´æµã€æ©¡èƒ¶ç­‹ã€é«˜å‹é”…ç­‰ï¼‰è®©éä¸“ä¸šäººå£«ç†è§£å…¶ç‰©ç†å›¾åƒã€‚]
    * **ğŸ¤” æ ¸å¿ƒéš¾ç‚¹/ç“¶é¢ˆ**ï¼š
        [ä¸€å¥è¯æŒ‡å‡ºç›®å‰é˜»ç¢è¿™ä¸ªæŠ€æœ¯å®ç°çš„æœ€å¤§å·¥ç¨‹æˆ–ç‰©ç†éšœç¢æ˜¯ä»€ä¹ˆï¼Ÿ]
    
    ---
    *Generated by FusionBot Â· Data Source: Crawler Level-2*
    """
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            print(f"å°è¯• {attempt+1} å¤±è´¥: {e}")
            time.sleep(5)
            
    return "âš ï¸ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API é…é¢ã€‚"

# --- 4. æ¨é€ ---
def send_wechat(content):
    if not SERVERCHAN_SENDKEY:
        return
    url = f"https://sctapi.ftqq.com/{SERVERCHAN_SENDKEY}.send"
    data = {"title": f"âš›ï¸ {datetime.date.today()} èšå˜æƒ…æŠ¥å±€", "desp": content}
    requests.post(url, data=data)

if __name__ == "__main__":
    news = get_fusion_news()
    job_data = search_jobs_deep_dive() # ä½¿ç”¨æ–°çš„äºŒçº§çˆ¬è™«å‡½æ•°
    report = generate_daily_report(news, job_data)
    print(report)
    send_wechat(report)
