import os
import requests
import feedparser
import datetime
import time
import random
import hashlib
import google.generativeai as genai
from bs4 import BeautifulSoup
from time import mktime
from duckduckgo_search import DDGS

# --- é…ç½®éƒ¨åˆ† ---
SERVERCHAN_SENDKEY = os.environ.get("SERVERCHAN_SENDKEY")
GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY") 

# åˆå§‹åŒ– Gemini (ä½¿ç”¨ 1.5-flash)
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        print(f"Gemini é…ç½®å‡ºé”™: {e}")
else:
    print("è­¦å‘Š: æœªé…ç½® GOOGLE_API_KEY")

# --- è¾…åŠ©å‡½æ•°ï¼šç½‘é¡µå†…å®¹æå–å™¨ (å€Ÿé‰´ WorkAggregation æ€è·¯) ---
def fetch_webpage_content(url):
    """
    æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—® URLï¼Œæå–ç½‘é¡µæ­£æ–‡æ–‡æœ¬
    """
    try:
        # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«ç®€å•çš„åçˆ¬è™«æ‹¦æˆª
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5'
        }
        # è®¾ç½® 10ç§’ è¶…æ—¶ï¼Œé˜²æ­¢å¡æ­»
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # æ£€æŸ¥ 404/500 é”™è¯¯
        
        # ä½¿ç”¨ BeautifulSoup è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ script, style ç­‰æ— ç”¨æ ‡ç­¾
        for script in soup(["script", "style", "nav", "footer", "header", "iframe"]):
            script.extract()
            
        # è·å–çº¯æ–‡æœ¬
        text = soup.get_text(separator=' ', strip=True)
        
        # æˆªå–å‰ 2500 ä¸ªå­—ç¬¦ (é˜²æ­¢ Token çˆ†ç‚¸ï¼Œé€šå¸¸ JD éƒ½åœ¨å‰é¢)
        return text[:2500]
        
    except Exception as e:
        print(f"  - è®¿é—®é“¾æ¥å¤±è´¥ {url}: {e}")
        return None # æŠ“å–å¤±è´¥è¿”å›ç©º

# --- 1. è·å–æ–°é—» ---
def get_fusion_news():
    print("æ­£åœ¨æŠ“å–æ–°é—»...")
    rss_url = "https://news.google.com/rss/search?q=Nuclear+Fusion+when:48h&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(rss_url)
        news_items = []
        for entry in feed.entries[:8]: 
            published_time_str = "æœªçŸ¥æ—¶é—´"
            if hasattr(entry, 'published_parsed'):
                dt = datetime.datetime.fromtimestamp(mktime(entry.published_parsed))
                published_time_str = dt.strftime('%Y-%m-%d %H:%M')
            
            news_items.append(f"- {entry.title} (Time: {published_time_str}) [Link: {entry.link}]")
            
        return "\n".join(news_items) if news_items else "è¿‡å»48å°æ—¶æ— é‡å¤§æ–°é—»ã€‚"
    except Exception as e:
        return f"æ–°é—»æŠ“å–å¤±è´¥: {e}"

# --- 2. æ·±åº¦èŒä½æŒ–æ˜ (Search + Visit) ---
def search_internships():
    print("æ­£åœ¨å¯åŠ¨æ·±åº¦èŒä½æŒ–æ˜æœº...")
    
    # ç­–ç•¥ç»„åˆï¼šæ··åˆæœç´¢ï¼Œè¯•å›¾æ‰¾åˆ°å…·ä½“çš„æ‹›è˜é¡µé¢
    search_strategies = [
        'site:iter.org "job" OR "internship" -filetype:pdf',
        'site:cfs.energy "careers" OR "jobs"',
        'site:helionenergy.com "openings"',
        'site:pppl.gov "jobs"',
        '"nuclear fusion" "we are hiring" -news',
        '"plasma physics" internship 2025'
    ]
    
    query = random.choice(search_strategies)
    print(f"æœ¬æ¬¡é›·è¾¾é”å®š: {query}")

    try:
        # 1. å…ˆæœé“¾æ¥
        # å‡å°‘æ•°é‡åˆ° 4 ä¸ªï¼Œå› ä¸ºåé¢è¦ä¸€ä¸ªä¸ªè®¿é—®ï¼Œå¤ªæ…¢äº†ä¼šè¶…æ—¶
        results = DDGS().text(query, max_results=4)
        
        if not results:
            return "DuckDuckGo æœªå‘ç°é›·è¾¾ä¿¡å·ï¼Œå»ºè®®æ‰‹åŠ¨æ£€æŸ¥ã€‚"

        processed_jobs = []
        for item in results:
            title = item.get('title', 'No Title')
            link = item.get('href', '#')
            snippet = item.get('body', '')
            
            print(f"å‘ç°çº¿ç´¢: {title}ï¼Œæ­£åœ¨æ´¾é£çˆ¬è™«æ·±å…¥ä¾¦å¯Ÿ...")
            
            # 2. ã€æ ¸å¿ƒå‡çº§ã€‘ç‚¹è¿›å»çœ‹ï¼
            # è°ƒç”¨ä¸Šé¢çš„ fetch å‡½æ•°å»æŠ“ç½‘é¡µæ­£æ–‡
            full_content = fetch_webpage_content(link)
            
            if full_content:
                # å¦‚æœæŠ“åˆ°äº†æ­£æ–‡ï¼Œå°±å–‚ç»™ AI æ­£æ–‡
                content_to_use = f"ã€ç½‘é¡µæ­£æ–‡æŠ“å–ã€‘: {full_content}"
            else:
                # å¦‚æœæŠ“å–å¤±è´¥ï¼ˆæ¯”å¦‚è¢«åçˆ¬ï¼‰ï¼Œå›é€€åˆ°ä½¿ç”¨æ‘˜è¦
                content_to_use = f"ã€ä»…æ‘˜è¦ã€‘: {snippet}"
            
            processed_jobs.append(f"SOURCE_URL: {link}\nTITLE: {title}\nCONTENT: {content_to_use}\n---")
            
            # ç¤¼è²Œæ€§å»¶æ—¶ï¼Œé˜²æ­¢è¯·æ±‚å¤ªå¿«è¢«å°
            time.sleep(2)
            
        return "\n".join(processed_jobs)

    except Exception as e:
        print(f"æŒ–æ˜æœºæ•…éšœ: {e}")
        return f"èŒä½æ‰«ææ¨¡å—æš‚æ—¶ä¼‘çœ : {e}"

# --- 3. ç”Ÿæˆæ—¥æŠ¥ (Prompt é€‚é…é•¿æ–‡æœ¬) ---
def generate_daily_report(news_text, internship_text):
    print("æ­£åœ¨ç”Ÿæˆ AI æ—¥æŠ¥...")
    today_str = datetime.date.today().strftime('%Y-%m-%d')

    fusion_topics = [
        "åŠ³æ£®åˆ¤æ®", "åº“ä»‘ç¢°æ’", "Qå€¼", "ä¸‰é‡ç§¯", "MHDä¸ç¨³å®šæ€§", "é˜¿å°”èŠ¬æ³¢", 
        "æ‰˜å¡é©¬å…‹", "ä»¿æ˜Ÿå™¨", "çƒå½¢æ‰˜å¡é©¬å…‹", "ååœºç®ç¼©", "Z-Pinch", "ICF",
        "ç¬¬ä¸€å£ææ–™", "é’¨", "é“", "åæ»¤å™¨", "æ°šå¢æ®–æ¯”", "é”‚é“…åŒ…å±‚", "ä¸­å­è¾ç…§", 
        "NBIåŠ çƒ­", "ICRH", "ECRH", "Hæ¨¡å¼", "ELMs", "é”¯é½¿æŒ¯è¡", 
        "ITER", "CFS SPARC", "Helion", "General Fusion", "HL-3", "NIF"
    ]
    
    date_hash = int(hashlib.sha256(today_str.encode('utf-8')).hexdigest(), 16)
    today_topic = fusion_topics[date_hash % len(fusion_topics)]

    prompt = f"""
    ä½ æ˜¯ä¸€ä½**æ ¸èšå˜æƒ…æŠ¥å±€ç‰¹å·¥**ã€‚è¯·ç”Ÿæˆ {today_str} çš„æ—¥æŠ¥ã€‚
    
    ---
    ### 1. News Data
    {news_text}
    
    ### 2. Job Intel (æ·±åº¦æŠ“å–æ•°æ®)
    *(ä»¥ä¸‹æ•°æ®åŒ…å«äº†çˆ¬è™«ç›´æ¥ä»ç½‘é¡µæŠ“å–çš„æ­£æ–‡ã€‚è¯·å¿½ç•¥ç½‘é¡µå¯¼èˆªæ ç­‰æ‚è®¯ï¼Œé‡ç‚¹æå–èŒä½æè¿°ã€è¦æ±‚ã€‚)*
    {internship_text}
    
    ### 3. Topic: {today_topic}
    
    ---
    ### è¾“å‡ºè¦æ±‚ (Markdown)
    
    # âš›ï¸ èšå˜æƒ…æŠ¥å±€ | {today_str}
    
    ## ğŸ“° 1. Fusion Frontiers
    *(ç­›é€‰ 4-5 æ¡æ–°é—»)*
    * **[ä¸­æ–‡æ ‡é¢˜]**
        * ğŸ•’ **Time**: [æ—¶é—´]
        * ğŸš€ **Significance**: [ç‚¹è¯„]
        * ğŸ”— [ç‚¹å‡»é˜…è¯»åŸæ–‡]({'{link}'}) 
    
    ## ğŸ¯ 2. Career Radar (æ·±åº¦ä¾¦å¯Ÿ)
    *(æŒ‡ä»¤ï¼šæˆ‘å·²é€šè¿‡çˆ¬è™«æŠ“å–äº†ç½‘é¡µæ­£æ–‡ã€‚è¯·æ ¹æ®ã€ç½‘é¡µæ­£æ–‡æŠ“å–ã€‘çš„å†…å®¹ï¼ŒåƒçŒå¤´ä¸€æ ·è¯¦ç»†åˆ†æã€‚)*
    *(å¦‚æœæŠ“å–å†…å®¹åŒ…å« "Apply"ã€"Requirements"ã€"Responsibilities" ç­‰å¹²è´§ï¼Œè¯·é‡ç‚¹åˆ—å‡ºã€‚)*
    *(å¦‚æœæŠ“å–å†…å®¹çœ‹èµ·æ¥æ˜¯å¾ˆå¤šèŒä½çš„åˆ—è¡¨é¡µï¼Œè¯·æ€»ç»“â€œè¯¥æœºæ„æ­£åœ¨æ‹›è˜å“ªäº›æ–¹å‘çš„äººæ‰â€ã€‚)*
    
    * ğŸ¢ **[æœºæ„/èŒä½åç§°]**
        * ğŸ“ **æ·±åº¦æƒ…æŠ¥**: [ä»æ­£æ–‡ä¸­æå–ï¼šå…·ä½“åœ¨åšä»€ä¹ˆé¡¹ç›®ï¼Ÿæ¶‰åŠä»€ä¹ˆç‰©ç†/å·¥ç¨‹éš¾é¢˜ï¼Ÿ]
        * ğŸ› ï¸ **é€šç¼‰ä»¤**: [ä»æ­£æ–‡ä¸­æå–ï¼šç¡¬æ€§è¦æ±‚æ˜¯ä»€ä¹ˆï¼ŸPhDï¼ŸPythonï¼ŸC++ï¼Ÿ]
        * ğŸ”— [ç‚¹å‡»ç›´è¾¾å®˜ç½‘]({'{link}'})
    
    ## ğŸ§  3. Deep Dive: {today_topic}
    * **ä»Šæ—¥è¯æ¡ï¼š{today_topic}**
    * **ğŸ§ ç¡¬æ ¸è§£æ**ï¼š[200å­—]
    * **ğŸ äººè¯ç‰ˆ**ï¼š[ç”Ÿæ´»æ¯”å–»ï¼Œ150å­—]
    * **ğŸ¤” ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**ï¼š[ä¸€å¥è¯]
    
    ---
    *Generated by FusionBot Â· Topic: {today_topic}*
    """
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            print(f"å°è¯• {attempt+1} å¤±è´¥: {e}")
            time.sleep(5)
            
    return "âš ï¸ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API é…é¢ã€‚"

# --- 4. æ¨é€ ---
def send_wechat(content):
    if not SERVERCHAN_SENDKEY:
        return
    url = f"https://sctapi.ftqq.com/{SERVERCHAN_SENDKEY}.send"
    data = {"title": f"âš›ï¸ {datetime.date.today()} èšå˜æƒ…æŠ¥å±€", "desp": content}
    requests.post(url, data=data)

if __name__ == "__main__":
    news = get_fusion_news()
    internships = search_internships()
    report = generate_daily_report(news, internships)
    print(report)
    send_wechat(report)
